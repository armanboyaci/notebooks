{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume we have a classifier that produces a score between 0 and 1 for the probability of a particular loan application being fraudulent. In this scenario: a) what are false positives, b) what are false negatives, and c) what are the trade-offs between them in terms of dollars and how should the model be weighted accordingly?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification:\n",
    "\n",
    "A classifier is a function $f : \\mathcal{X} \\to \\mathcal{Y}$ from an input space $\\mathcal{X}$, e.g. load applications, to the space of categorical (or binary) outcomes $\\mathcal{Y} = \\{ 0, 1 \\}$, e.g. the application being fradulent or not, where $y = 1$ denotes a fradulent application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer:\n",
    "\n",
    "Given an application $x \\in \\mathcal{X}$, and its fraudulence $y \\in \\mathcal{Y}$, a classifier $f$ produces a result as\n",
    "\n",
    "\\begin{equation*}\n",
    "    \\hat{y} = f(x)\n",
    "\\end{equation*}\n",
    "\n",
    "a) A false positive is the case where $y=0$ and $\\hat{y}=1$.\n",
    "\n",
    "b) A false negative is the case where $y=1$ and $\\hat{y}=0$.\n",
    "\n",
    "c) Assume that the cost of accepting a fraudulent load application is $C_F$ and the cost of rejecting an honest application is $C_H$.\n",
    "\n",
    "For a given classifier $f$, \n",
    "\n",
    "|          | Honest  | Fraud |\n",
    "| -------- | ------- | ----- |\n",
    "| $f(x) = \\text{Honest}$  | $N_{H,H}$    | $N_{F,H}$  |\n",
    "| $f(x) = \\text{Fraud}$   | $N_{H,F}$    | $N_{F,F}$  |\n",
    "\n",
    "\\begin{align*}\n",
    "    \\text{Sensitivity} &= \\frac{N_{F,F}}{N_{F,F} + N_{F,H}} \\\\\n",
    "    &= \\text{Probability of guessing that an application is fraudulent when the application is fraudulent.}\n",
    "\\end{align*}\n",
    "\n",
    "\\begin{align*}\n",
    "    \\text{Specificity} &= \\frac{N_{H,H}}{N_{H,H} + N_{H,F}} \\\\\n",
    "    &= \\text{Probability of guessing that an application is not fraudulent when the application is not fraudulent.}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then the total cost is \n",
    "\n",
    "\\begin{align*}\n",
    "    C &= \\left( 1 - \\frac{N_{F,F}}{N_{F,F} + N_{F,H}} \\right) C_F \\, + \\left( 1- \\frac{N_{H,H}}{N_{H,H} + N_{H,F}} \\right) C_H \\\\\n",
    "    &= \\left( \\frac{N_{F,H}}{N_{F,F} + N_{F,H}} \\right) C_F \\, + \\left( \\frac{N_{H,F}}{N_{H,H} + N_{H,F}} \\right) C_H \\\\\n",
    "    &= \\frac{N_{F,H} C_F}{N_{F,F} + N_{F,H}} + \\frac{N_{H,F} C_H }{N_{H,H} + N_{H,F}}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume that an application $x \\in \\mathcal{X}$ is mapped to some real number $r \\in \\mathbb{R}$. A simple classifer can divide this space using a single threshold value $\\tau$ such that\n",
    "\n",
    "\\begin{align*}\n",
    "    f(r) = \\begin{cases}\n",
    "        \\text{Honest}, \\quad \\text{if} \\, \\, r \\leq \\tau \\\\\n",
    "        \\text{Fraud}, \\, \\, \\quad \\text{if} \\, \\, r > \\tau \\\\\n",
    "    \\end{cases}\n",
    "\\end{align*}\n",
    "\n",
    "Notice that increasing $\\tau$ corresponds to increasing \n",
    "\n",
    "\\begin{align*}\n",
    "    \\frac{N_{H,H} + N_{F,H}}{N_{H,H} + N_{F,H} + N_{H,F} + N_{F,F}}\n",
    "\\end{align*}\n",
    "\n",
    "since a larger proportion of all applications will be classified as an $\\text{Honest}$ application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
